{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704ff249",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"https://soudevcon.com.br/wp-content/uploads/2025/01/logo-soudevcon-1000-branco.png\" width=\"300px\"/><br><br>\n",
    "</p>\n",
    "\n",
    "## SouDevCon - 2025\n",
    "### A Celebração das Linguagens de Programação!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b16c687",
   "metadata": {},
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML('<style>pre {white-space: pre !important;}</style>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df0fd6",
   "metadata": {},
   "source": [
    "# Apache Spark - Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa1f48",
   "metadata": {},
   "source": [
    "### [Apache Spark](https://spark.apache.org/)\n",
    "\n",
    "Apache Spark é uma plataforma de computação em *cluster* que fornece uma API para programação distribuída para processamento de dados em larga escala, semelhante ao modelo *MapReduce*, mas projetada para ser rápida para consultas interativas e algoritmos iterativos.\n",
    "\n",
    "O Spark permite que você distribua dados e tarefas em clusters com vários nós. Imagine cada nó como um computador separado. A divisão dos dados torna mais fácil o trabalho com conjuntos de dados muito grandes porque cada nó funciona processa apenas uma parte parte do volume total de dados.\n",
    "\n",
    "O Spark é amplamente utilizado em projetos analíticos nas seguintes frentes:\n",
    "\n",
    "- Preparação de dados\n",
    "- Modelos de machine learning\n",
    "- Análise de dados em tempo real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc35317-0f63-4843-8553-2276a36fbaae",
   "metadata": {},
   "source": [
    "## [SparkSession](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.html)\n",
    "\n",
    "O ponto de entrada para programar o Spark com a API Dataset e DataFrame.\n",
    "\n",
    "Uma SparkSession pode ser utilizada para criar DataFrames, registrar DataFrames como tabelas, executar consultas SQL em tabelas, armazenar em cache e ler arquivos parquet. Para criar uma SparkSession, use o seguinte padrão de construtor:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ea413-9688-4257-8d33-d31951605bea",
   "metadata": {},
   "source": [
    "## DataFrames com Spark\n",
    "\n",
    "\n",
    "### Interfaces Spark\n",
    "\n",
    "Existem três interfaces principais do Apache Spark que você deve conhecer: Resilient Distributed Dataset, DataFrame e Dataset.\n",
    "\n",
    "- **Resilient Distributed Dataset**: A primeira abstração do Apache Spark foi o Resilient Distributed Dataset (RDD). É uma interface para uma sequência de objetos de dados que consiste em um ou mais tipos localizados em uma coleção de máquinas (um cluster). Os RDDs podem ser criados de várias maneiras e são a API de “nível mais baixo” disponível. Embora esta seja a estrutura de dados original do Apache Spark, você deve se concentrar na API DataFrame, que é um superconjunto da funcionalidade RDD. A API RDD está disponível nas linguagens Java, Python e Scala.\n",
    "\n",
    "- **DataFrame**: Trata-se de um conceito similar ao DataFrame que você pode estar familiarizado como o pacote pandas do Python e a linguagem R . A API DataFrame está disponível nas linguagens Java, Python, R e Scala.\n",
    "\n",
    "- **Dataset**: uma combinação de DataFrame e RDD. Ele fornece a interface digitada que está disponível em RDDs enquanto fornece a conveniência do DataFrame. A API Dataset está disponível nas linguagens Java e Scala.\n",
    "\n",
    "Em muitos cenários, especialmente com as otimizações de desempenho incorporadas em DataFrames e Datasets, não será necessário trabalhar com RDDs. Mas é importante entender a abstração RDD porque:\n",
    "\n",
    "- O RDD é a infraestrutura subjacente que permite que o Spark seja executado com tanta rapidez e forneça a linhagem de dados.\n",
    "\n",
    "- Se você estiver mergulhando em componentes mais avançados do Spark, pode ser necessário usar RDDs.\n",
    "\n",
    "- As visualizações na Spark UI fazem referência a RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30c854b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b011bb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3a1181a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "__Licensa__\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Cc_by-nc_icon.svg/1200px-Cc_by-nc_icon.svg.png\" width=\"20%\"/>\n",
    "\n",
    "\n",
    "\n",
    "*This work is licensed under the Creative Commons Attribution-NonCommercial 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
